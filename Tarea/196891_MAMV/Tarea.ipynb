{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea Apache Spark\n",
    ">Importa las librerías necesarias dónde sea necesario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession\n",
    ">Crea un SparkSession para comenzar la tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Configurar la aplicación Spark\n",
    "app_name = \"tarea\"\n",
    "\n",
    "# Crear una instancia de SparkSession\n",
    "spark = SparkSession.builder.appName(app_name).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un DataFrame\n",
    ">Lee el csv datosTarea.csv, mételo a un DF y muéstralo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.csv(\"/datosTarea.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtro de datos\n",
    ">Consigue todas las empresas que empiecen con 'M' y tengan entre 4000 y 7000 empleados. Sólo muestra los nombres y el número de empleados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = df.filter((df['Name'].startswith('M')) & (df[\"Number of Employees\"].between(4000, 7000))).select(\"Name\", \"Number of Employees\")\n",
    "\n",
    "resp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Consigue todos los países que no inicien con las letras 'b', 's' y 'm', pero que tampoco tengan un netword mayor a 500000. Muestra el nombre de la compañía, el país y el networth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los países que no comienzan con 'b', 's' o 'm' y tienen un networth menor o igual a 500000\n",
    "result_df = df.filter(\n",
    "    (~col(\"Country\").startswith(\"b\")) &\n",
    "    (~col(\"Country\").startswith(\"s\")) &\n",
    "    (~col(\"Country\").startswith(\"m\")) &\n",
    "    (col(\"Networth\") <= 500000)\n",
    ")\n",
    "\n",
    "# Mostrar el resultado\n",
    "result_df.select(\"Name\", \"Country\", \"Networth\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones\n",
    "Crea una función con @pandas_udf que que le reste a los profits la media en cada renglón. Crea una nueva columna que muestre los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_mean(pdf):\n",
    "    return pdf.assign(profit_minus_mean=pdf['profit'] - pdf['profit'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping data\n",
    ">Agrupa por industry y muestra cuáles son las empresas con el profit más alto. Muestra los primeros tres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, rank\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Agrupar por industry y encontrar las empresas con el profit más alto\n",
    "result_df = df.groupBy(\"Industry\").agg({\"Profit\": \"max\"}).withColumnRenamed(\"max(Profit)\", \"max_profit\")\n",
    "\n",
    "# Ordenar por profit de manera descendente y mostrar los tres primeros\n",
    "result_df = result_df.orderBy(desc(\"max_profit\")).limit(3)\n",
    "\n",
    "# Mostrar el resultado\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Agrupa por industry y calcula el promedio de empleados que tienen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "# Agrupar por industry y calcular el promedio de empleados\n",
    "result_df = df.groupBy(\"Industry\").agg(avg(\"Number of Employees\"))\n",
    "\n",
    "# Mostrar el resultado\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL\n",
    ">Usando Spark SQL, obtén cuántas empresas se fundaron despúes del 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"empresa_table\")\n",
    "query = \"\"\"\n",
    "    SELECT COUNT(*) as count_empresas\n",
    "    FROM empresa_table\n",
    "    WHERE Founded > 2000\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Regresión Lineal\n",
    ">Con número de empleados, networth y stock price, obtén una predicción del profit a través de una regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Ensamblar las características en un vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Number of employees\", \"Networth\", \"stock_price\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "df_assembled = assembler.transform(df)\n",
    "# Seleccionar las columnas relevantes\n",
    "data = df_assembled.select(\"features\", \"Profit\")\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# Configurar el modelo de regresión lineal\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Profit\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Una vez que obtengas los resultados, a través del api de pandas, conviértelo en un pandas on spark DataFrame y pásalo a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el DataFrame de PySpark a un DataFrame de pandas\n",
    "pandas_df = predictions.select(\"features\", \"profit\", \"prediction\").toPandas()\n",
    "\n",
    "# Guardar el DataFrame de pandas como un archivo CSV\n",
    "pandas_df.to_csv(\"resultados.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
